{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32ea86de-6031-4a81-90dd-305a9942273b",
   "metadata": {},
   "source": [
    "# Fetch and Process Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "be480ae6-7721-4a9e-899a-bbf9a8f9d841",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from utils.data_processing import *\n",
    "from utils.preprocessing import *\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "3d257ce2-e58c-4ac3-a4fe-488c16b2fd0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42d7fd02cbb9466db4461ec028556e23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/159571 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d3b7ec0d2cb408b9d1a429f42aeac97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/159571 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Tox: 16225\n",
      "Num Not Tox: 16224\n"
     ]
    }
   ],
   "source": [
    "training_x, training_y = data_processing('./data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "c699ea5c-f7e1-484d-9c29-cc7c0fec504c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27025c1772b64612a563056d060ac89e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32449 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preprocessed_x = preprocess(training_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694d7865-60ee-43f3-9346-905685e18287",
   "metadata": {},
   "source": [
    "## Encode Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "b3b0c0cf-af4f-48c7-94aa-7c980d80954d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "906c7027-3d6e-4133-bb65-946b683ae8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildVocab(training_data, tokenised=None, feedforward=None):\n",
    "    \n",
    "    vocab={}\n",
    "    word_counts = {}\n",
    "    vocab['<sos>'] = 1\n",
    "    vocab['<eos>'] = 2\n",
    "    vocab['<oov>'] = 3\n",
    "    \n",
    "    processed_lines = []\n",
    "    \n",
    "    for line in training_data:\n",
    "        \n",
    "        tokens = line.split(' ')\n",
    "        \n",
    "        for token in tokens:\n",
    "            \n",
    "            if token not in vocab:\n",
    "                vocab[token] = len(vocab)\n",
    "                word_counts[token] = 1\n",
    "                \n",
    "            else:\n",
    "                word_counts[token] += 1\n",
    "        \n",
    "        if not feedforward:\n",
    "            tokens.insert(0, '<sos>')\n",
    "            tokens.append('<eos>')\n",
    "        processed_lines.append(tokens)\n",
    "    \n",
    "    return vocab, word_counts, processed_lines\n",
    "\n",
    "def encodeData(data, vocab, test=None):\n",
    "    \n",
    "    encoded_data = []\n",
    "    \n",
    "    for line in data:\n",
    "            \n",
    "        encoded_line = []\n",
    "        \n",
    "        for token in line:\n",
    "            \n",
    "            if test==True:\n",
    "                if token not in line:\n",
    "                    encoded_line.append(vocab['<oov'])\n",
    "                    continue\n",
    "                    \n",
    "            encoded_line.append(vocab[token])\n",
    "        \n",
    "        encoded_data.append(torch.LongTensor(encoded_line))\n",
    "        \n",
    "    return pad_sequence(encoded_data, batch_first=True)\n",
    "\n",
    "def encodeMultiHot(line, vocab, test=None):\n",
    "    \n",
    "\n",
    "    empty_vec = np.zeros(len(vocab))\n",
    "\n",
    "    for token in line:\n",
    "\n",
    "        if token in vocab:\n",
    "            empty_vec[vocab[token]] += 1\n",
    "\n",
    "\n",
    "    return np.array(empty_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "9ba781b1-903b-48d9-a458-0c5ff22b4345",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "2f5ed28b-7892-4834-bf2c-2f1c4c7f0e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hottyY(int):\n",
    "    \n",
    "    if int == 0:\n",
    "        return torch.Tensor([1, 0])\n",
    "    elif int == 1:\n",
    "        return torch.Tensor([0,1])\n",
    "\n",
    "class netDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, x,y=None, test=None):\n",
    "        self.x = x\n",
    "        self.test = test\n",
    "        if not self.test:\n",
    "            self.y = [hottyY(i) for i in y]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        x = encodeMultiHot(self.x[idx], vocab)\n",
    "                    \n",
    "        if not self.test:\n",
    "            y = self.y[idx]\n",
    "        \n",
    "            return x, y\n",
    "        \n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73358da-4f40-4b0f-a9b5-90b1db97712f",
   "metadata": {},
   "source": [
    "## Feed Forward Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "0cd17a7e-3b42-4068-aa66-ac3f7ac19569",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, word_counts, post_processed_lines = buildVocab(preprocessed_x, feedforward=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "26cadef2-b259-4e69-acc5-3f33a770742f",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = netDataset(post_processed_lines, training_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "1bfebf34-df78-42e6-a0f7-7a2f248b408f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "b3dad57d-7919-4641-a1d7-eb14dac40584",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(FeedForward, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.fc1 = nn.Linear(self.input_size, self.hidden_size)\n",
    "        self.fc2 = nn.Linear(self.hidden_size, 2)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        out1 = self.activation(self.fc1(x))\n",
    "        out2 = self.fc2(out1)\n",
    "        return self.sigmoid(out2)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f10df2-1c63-4365-bdd5-d1b10e50c405",
   "metadata": {},
   "source": [
    "### Network Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "49c79654-c73a-4d66-88d4-36f252447335",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = len(vocab)\n",
    "hidden_size = 128\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "c74103b3-ede0-4fc5-94e9-796d39c48f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FeedForward(\n",
      "  (fc1): Linear(in_features=113173, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=2, bias=True)\n",
      "  (activation): ReLU()\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = FeedForward(input_size, hidden_size)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "451a58f9-bd5a-420f-8f19-a8747c089acb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0ae09e47-da4e-4fe6-8169-2912b228de4c",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "ad3b25fb-619b-406b-9813-bcba6dbd077a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training_dataloader = DataLoader(training_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "fc28b8a3-395d-4bfa-9c32-a25d1682ef27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6e51bb6f5e242eb8531cd1e51423be0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "784345739bff4463a97fadea7dcdd8f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 0:   0%|          | 0/32449 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Loss: 0.03970193862915039\n",
      "Avg Loss: 0.7896829358772514\n",
      "Avg Loss: 1.2191842275552984\n",
      "Avg Loss: 1.0847577477416863\n",
      "Avg Loss: 1.2292711912322443\n",
      "Avg Loss: 1.071494076507832\n",
      "Avg Loss: 1.1650097880417245\n",
      "Avg Loss: 1.0856247607517355\n",
      "Avg Loss: 1.1210545919660524\n",
      "Avg Loss: 1.1650303669955213\n",
      "Avg Loss: 1.1377810368517718\n",
      "Avg Loss: 1.1365979783680347\n",
      "Avg Loss: 1.117551064866927\n",
      "Avg Loss: 1.0787048085911808\n",
      "Avg Loss: 1.0776626692941258\n",
      "Avg Loss: 1.0799770955404426\n",
      "Avg Loss: 1.035887675213235\n",
      "Avg Loss: 1.0840133428201866\n",
      "Avg Loss: 1.086046906533517\n",
      "Avg Loss: 1.058770263394971\n",
      "Avg Loss: 1.0465514266616178\n",
      "Avg Loss: 1.0233041491314303\n",
      "Avg Loss: 1.0138812563383681\n",
      "Avg Loss: 1.02582770429888\n",
      "Avg Loss: 1.0269136585128602\n",
      "Avg Loss: 1.0059384535667337\n",
      "Avg Loss: 1.017490840225509\n",
      "Avg Loss: 1.0084980741953062\n",
      "Avg Loss: 1.0107755950780821\n",
      "Avg Loss: 0.9925380884009708\n",
      "Avg Loss: 0.9948563874104577\n",
      "Avg Loss: 1.0158533328191515\n",
      "Avg Loss: 1.0178937728838446\n",
      "Avg Loss: 1.0201689890973835\n",
      "Avg Loss: 1.0310412382308969\n",
      "Avg Loss: 1.014371957937343\n",
      "Avg Loss: 1.0131774793552262\n",
      "Avg Loss: 1.0186533870184833\n",
      "Avg Loss: 1.0090788191288271\n",
      "Avg Loss: 1.0169146987680546\n",
      "Avg Loss: 1.0279993580002076\n",
      "Avg Loss: 1.0150627489542556\n",
      "Avg Loss: 1.0237912567314411\n",
      "Avg Loss: 1.022611202744025\n",
      "Avg Loss: 1.0285988622331226\n",
      "Avg Loss: 1.0293410251890083\n",
      "Avg Loss: 1.0937852841572944\n",
      "Avg Loss: 1.2141242302125541\n",
      "Avg Loss: 1.3138247826793517\n",
      "Avg Loss: 1.4171089827116363\n",
      "Avg Loss: 1.4563663336795973\n",
      "Avg Loss: 1.4890830371023787\n",
      "Avg Loss: 1.5157340050980643\n",
      "Avg Loss: 1.4989284958277835\n",
      "Avg Loss: 1.4831129972570358\n",
      "Avg Loss: 1.4856926436866156\n",
      "Avg Loss: 1.4770199479375576\n",
      "Avg Loss: 1.4642657714025362\n",
      "Avg Loss: 1.44548613653503\n",
      "Avg Loss: 1.4294616267848494\n",
      "Avg Loss: 1.4160544916357332\n",
      "Avg Loss: 1.4092342057189964\n",
      "Avg Loss: 1.398601824675579\n",
      "Avg Loss: 1.3922750626474756\n",
      "Avg Loss: 1.376594393695043\n",
      "Avg Loss: 1.3746468625432404\n",
      "Avg Loss: 1.3576074039993484\n",
      "Avg Loss: 1.3410765670305882\n",
      "Avg Loss: 1.338685485132615\n",
      "Avg Loss: 1.3319656491102592\n",
      "Avg Loss: 1.3218665005211336\n",
      "Avg Loss: 1.3102913235834044\n",
      "Avg Loss: 1.305981871500721\n",
      "Avg Loss: 1.3000782124273917\n",
      "Avg Loss: 1.2977123705649223\n",
      "Avg Loss: 1.2970787794120153\n",
      "Avg Loss: 1.2865912457202024\n",
      "Avg Loss: 1.2844927271519049\n",
      "Avg Loss: 1.282448014134349\n",
      "Avg Loss: 1.2791141137328736\n",
      "Avg Loss: 1.2771877475101572\n",
      "Avg Loss: 1.266049970929062\n",
      "Average Loss: 1.264177173659359\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f463ab3a6234fe19753212a196298b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/32449 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Loss: 100.0\n",
      "Avg Loss: 89.40149625935162\n",
      "Avg Loss: 89.01373283395755\n",
      "Avg Loss: 89.25895087427143\n",
      "Avg Loss: 89.1630231105559\n",
      "Avg Loss: 89.28035982008996\n",
      "Avg Loss: 89.067055393586\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [278]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(out, y)\n\u001b[0;32m     15\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 17\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m idx \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m400\u001b[39m  \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\optim\\optimizer.py:88\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[1;32m---> 88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[1;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\optim\\sgd.py:144\u001b[0m, in \u001b[0;36mSGD.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    141\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    142\u001b[0m             momentum_buffer_list\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmomentum_buffer\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m--> 144\u001b[0m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msgd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    145\u001b[0m \u001b[43m      \u001b[49m\u001b[43md_p_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    146\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmomentum_buffer_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    147\u001b[0m \u001b[43m      \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    148\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    149\u001b[0m \u001b[43m      \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    150\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdampening\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdampening\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnesterov\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnesterov\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    152\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;66;03m# update momentum_buffers in state\u001b[39;00m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p, momentum_buffer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(params_with_grad, momentum_buffer_list):\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\optim\\_functional.py:194\u001b[0m, in \u001b[0;36msgd\u001b[1;34m(params, d_p_list, momentum_buffer_list, weight_decay, momentum, lr, dampening, nesterov, maximize)\u001b[0m\n\u001b[0;32m    191\u001b[0m         d_p \u001b[38;5;241m=\u001b[39m buf\n\u001b[0;32m    193\u001b[0m alpha \u001b[38;5;241m=\u001b[39m lr \u001b[38;5;28;01mif\u001b[39;00m maximize \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m-\u001b[39mlr\n\u001b[1;32m--> 194\u001b[0m \u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_\u001b[49m\u001b[43m(\u001b[49m\u001b[43md_p\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "avg_loss = 0\n",
    "model.train()\n",
    "for i in tqdm(range(num_epochs), total=num_epochs, desc='Training'):\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    losses = []\n",
    "    \n",
    "    idx = 0\n",
    "    for x,y in tqdm(training_dataset, desc=f'Epoch {i}'):\n",
    "        \n",
    "        x = torch.Tensor(x)\n",
    "        out = model(x)\n",
    "        \n",
    "        loss = criterion(out, y)\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "    \n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        if idx % 400  == 0:\n",
    "            tqdm.write(f'Avg Loss: {sum(losses) / len(losses)}')\n",
    "        \n",
    "        idx += 1\n",
    "    print(f'Average Loss: {sum(losses) / len(losses)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96951c47-18c3-4e5c-9dd0-52805f4fdaec",
   "metadata": {},
   "source": [
    "### Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "4cadb2d3-add7-47bd-8fae-ef7ae0dafc86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8ab520eb92b444c8ffc7143ed4fa679",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/153164 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "158ebca42c634dc78f8e9824f42172ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/153164 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "testing_raw = pd.read_csv('./data/test.csv')\n",
    "transformed_test, _ = processRawDataFromCSV(testing_raw, test=True)\n",
    "preprocessed_x = preprocess(transformed_test)\n",
    "#encoded_test = encodeMultiHot(post_processed_lines, vocab, test=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "34f52d01-dfd6-4ebd-bc65-6612b798d559",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_dataset = netDataset(encoded_train, test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "d52ffee4-6f74-4ce2-b1a9-b75ecacbeac9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeedForward(\n",
       "  (fc1): Linear(in_features=113173, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=2, bias=True)\n",
       "  (activation): ReLU()\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "1ca21d0a-b864-42ca-9b84-22eca8927e32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41f90f3ff4a145a7af3a34460cd91680",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3283 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [287]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m preds \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(testing_dataset):\n\u001b[0;32m      4\u001b[0m     pred \u001b[38;5;241m=\u001b[39m model(torch\u001b[38;5;241m.\u001b[39mTensor(i))\u001b[38;5;241m.\u001b[39margmax()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m      6\u001b[0m     preds\u001b[38;5;241m.\u001b[39mappend(pred)\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tqdm\\notebook.py:257\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    256\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 257\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(tqdm_notebook, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__iter__\u001b[39m():\n\u001b[0;32m    258\u001b[0m             \u001b[38;5;66;03m# return super(tqdm...) will not catch exception\u001b[39;00m\n\u001b[0;32m    259\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tqdm\\std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1192\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1195\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1196\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1197\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1198\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "Input \u001b[1;32mIn [284]\u001b[0m, in \u001b[0;36mnetDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[1;32m---> 21\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mencodeMultiHot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest:\n\u001b[0;32m     24\u001b[0m         y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my[idx]\n",
      "Input \u001b[1;32mIn [263]\u001b[0m, in \u001b[0;36mencodeMultiHot\u001b[1;34m(line, vocab, test)\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m vocab:\n\u001b[0;32m     60\u001b[0m         empty_vec[vocab[token]] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39marray(empty_vec)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "preds = []\n",
    "for i in tqdm(testing_dataset):\n",
    "    \n",
    "    pred = model(torch.Tensor(i)).argmax().item()\n",
    "    \n",
    "    preds.append(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5e4d9b-8b94-40cd-8446-e618a3f99b87",
   "metadata": {},
   "source": [
    "## Recurrant Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "8200084d-8aaf-47f0-8cd8-db8b1b4b6423",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "buildVocab() got an unexpected keyword argument 'test'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [117]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m vocab, word_counts, test_post_processed \u001b[38;5;241m=\u001b[39m \u001b[43mbuildVocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreprocessed_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: buildVocab() got an unexpected keyword argument 'test'"
     ]
    }
   ],
   "source": [
    "vocab, word_counts, test_post_processed = buildVocab(preprocessed_x, test=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "7b051277-b65a-4400-bd95-6d90c5f99ff5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5164, 0.5299], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_train = encodeMultiHot(post_processed_lines, vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f3ee97eb-3581-4c83-afd1-6b37e388a9dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1, 3, 4,  ..., 0, 0, 0]), 0)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "fd3823e7-dcda-45dd-8aaa-37d6ce819952",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7079, grad_fn=<BinaryCrossEntropyBackward0>)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8795c5d-cc9d-4864-ab24-70d13c6b007a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
